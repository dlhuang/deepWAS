---
#title: "DeepWAS: Directly integrating regulatory information into GWAS using deep learning"
#author: "GÃ¶kcen Eraslan, Janine Arloth, Jade Martins, Stella Iurato, Darina Czamara, Elisabeth B. Binder, Fabian J. Theis, Nikola S. Mueller"
#date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: false
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{DeepWAS: Directly integrating regulatory information into GWAS using deep learning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

###DeepWAS: Directly integrating regulatory information into GWAS using deep learning
<hr />


## Introduction

DeepWAS [project](http://biorxiv.org/content/early/2016/08/11/069096) showed that deep learning of regulatory variant effects can improve our understanding on diseases. In a proof-of-concept study, we applied the approach to a major depression disorder (MDD) case/control cohort to interrogate putative non-coding causal variants of MDD. The aim of this document is to guide you through the code we used in our study.

## Prerequisites

`glmnet`, `ggplot2`, `ggrepel`, `gplots` and `RColorBrewer` packages are used for the visualization, whereas `snpStats` was used to read PLINK-formatted genotype and phenotype files. If you want to run DeepSEA from scratch, please check out additional R helper files on the [Github repository](https://github.com/gokceneraslan/deepwas) to run DeepSEA from within the R code conveniently.

```{r loadpackages, warning=F, message=F}
#Bioconductor
library(snpStats)

#Visualization and reporting
library(ggplot2)
library(ggrepel)
library(RColorBrewer)
library(gplots)
library(printr) # install from https://github.com/yihui/printr

#Regression, parallelization and data tidying
library(glmnet)
library(parallel)
library(tidyr)
library(dplyr) #at least 0.5.0

set.seed(42)
```
## Analysis

### Deep learning

Deep-learning methods have been proposed to make highly accurate predictions of various chromatin features and transcription factor binding sites using the DNA sequences of a given genomic range. Here we use DeepSEA to assess the regulatory impact of each variant we genotyped (and imputed) from the case/control cohort. Since deep learning part takes time (~12 hours per 1M variants with a Titan X and a magnetic disk where I/O takes %90 of time), here we'll be using an existing file with variant e-values.

```{r rundeepsea}
# Run DeepSEA -------------------------------------------------------------

# Run deepsea or use existing file
if (file.exists('data/evalues.tsv')) {
  snp.eval <- read.delim('data/evalues.tsv', stringsAsFactors = F)
} else {
  source('rundeepsea.R')
  deepsea.dir <- ''

  outdir <- rundeepsea(snps, deepsea.dir)
  snp.eval <- get.significant.snps(snps, outdir)
}

head(snp.eval)
```

This is how the predictions look like. Below we see the e-value distribution of our variants. Note that only the variants with evalue < 5e-5 are shown and are considered in the rest of the analysis. The lowest e-value is 1e-6 since there are 1M snps in the background distribution of DeepSEA.


```{r visualizedeepsea}
snps <- unique(snp.eval$snp)
qplot(snp.eval$mineval,
      color=I('white'),
      bins=60, log='x',
      xlab='e-values (log10)') +
  theme_minimal()
```


Next step is to group variants per {cell line, chromatin feature, treatment} triplet (e.g. `X8988T.DNase.None`), which is called a functional unit in the paper. We can plot the distribution of number of variants we have in each of 919 groups.

```{r visualizedeepsea2}

snp.eval.split <- split(snp.eval$snp,
                        snp.eval$mineval.feature.uniq)

snp.eval.split.len <- sapply(snp.eval.split, length)

# Save names of DeepSEA features
features.uniq <- unique(snp.eval$mineval.feature.uniq)
feature.mapping <- unique(snp.eval[,c('mineval.feature.uniq', 'mineval.feature')])
features <- feature.mapping[match(features.uniq, feature.mapping$mineval.feature.uniq),
                            'mineval.feature']

# Visualize number of SNPs we have for each deepse feature
qplot(snp.eval.split.len, bins=100,
      color=I('white'),
      xlab='Model feature sizes') +
  theme_minimal()

```

Then we read the genotype and phenotype files. Here we have them in PLINK format but there are many other formats available. In addition, dosage files can also be used.


```{r readgenotypes}

# Read plink or dosage ----------------------------------------------------

pl <- read.plink('data/gwas')
ind.ids <- rownames(pl$genotypes)
covar <- read.delim('data/gwas.covar', stringsAsFactors = F)
pl$fam <- merge(pl$fam, covar) #merge extra covariates
covar <- cbind(age=pl$fam$age, sex=pl$fam$sex-1)
response <- as.factor(pl$fam$affected - 1)

# Remove missing cases, shouldn't be a problem with dosage data
cc.index <- complete.cases(as(pl$genotypes, 'numeric'), covar, response)
pl$genotypes <- pl$genotypes[cc.index,]
covar <- covar[cc.index,]
response <- response[cc.index]
ind.ids <- ind.ids[cc.index]
names(response) <- rownames(pl$genotypes)
```

### Multi-locus regression

Next step is to fit regularized logistic regression models on grouped set of variants to investigate the association between set of variants and the disease in a specific context of cell line, transcription factor (or chromatin feature) and treatment.

```{r performregression}
# Regression --------------------------------------------------------------

if (file.exists('data/models.Rds')) {
  cv.fits <- readRDS('data/models.Rds')
} else {
  cv.fits <- mclapply(features.uniq, function(feature) {
    print(feature)
    mat <- cbind(as(pl$genotypes[, snp.eval.split[[feature]]], 'numeric'),
                 covar)

    fit <- cv.glmnet(mat,
                     response,
                     nfolds = 100,
                     alpha=1,
                     family='binomial')
    fit
  }, mc.cores = 12)

  names(cv.fits) <- features.uniq
  saveRDS(cv.fits, 'data/models.Rds')
}
```

This is done via `glmnet` package. A LASSO model is fitted to each of 919 groups of variants with the same response variable which is the binary disease status. `lambda` parameter of LASSO is determined via 100-fold cross-validation.

Let's now plot the distribution of the number non-zero coefficients in each model:

```{r visualizevariableselection, warning=F, message=F}

get.nonzero.coef <- function(fit, exclude=c('(Intercept)', 'age', 'sex')) {
  co <- coef(fit, s = fit$lambda.1se)
  l <- rownames(co)[which(co != 0)]
  setdiff(l, exclude)
}

fits.coeff <- lapply(cv.fits, get.nonzero.coef)
fits.coeff.len <- sapply(fits.coeff, length)
#qplot(fits.coeff.len, color=I('white'),
#      xlab='Number of non-zero coefficients', binwidth=1) + theme_minimal()
plot(table(fits.coeff.len), xlab='Number of non-zero coefficients', ylab='count')
```

### Visualization of variable selection

```{r visualizevariableselection2}
snp.eval.split.len <- snp.eval.split.len[match(names(fits.coeff.len),
                                               names(snp.eval.split.len)) ]

num.variant.x.cutoff <- quantile(ecdf(snp.eval.split.len), .996)
num.variant.y.cutoff <- quantile(ecdf(fits.coeff.len), .995)

num.variant.df <- data.frame(snp.eval.split.len=snp.eval.split.len,
                             fits.coeff.len=fits.coeff.len,
                             model=names(fits.coeff.len), stringsAsFactors = F)

ggplot(num.variant.df, aes(snp.eval.split.len, fits.coeff.len)) +
  geom_point() +
  geom_label_repel(data=subset(num.variant.df, snp.eval.split.len > num.variant.x.cutoff |
                                 fits.coeff.len > num.variant.y.cutoff),
                   aes(label=model)) +
  labs(x='Number of variants in models',
       y='Number of non-zero coefficients in fitted models') +
  theme_minimal()

```

### Model selection

Here we compare our LASSO models with other LASSO models that are fitted to the same covariates but with a shuffled response column. That shows us how significantly our model outperforms models fitted on random data.

```{r performmodelselection}
# Model selection ---------------------------------------------------------

if (file.exists('data/permutation_models.Rds') &&
    file.exists('data/permutation_models_devratios.Rds') &&
    file.exists('data/models_devratios.Rds')) {

  #U <- readRDS('data/permutation_models.Rds')
  D <- readRDS('data/permutation_models_devratios.Rds')
  main.model.dev.ratio <- readRDS('data/models_devratios.Rds')
} else {

  # Let's fit models on shuffled genotypes
  nperm <- 1000
  perm.index <- sapply(seq_len(nperm), function(x)sample(ind.ids))
  stopifnot(ncol(perm.index) == nperm)

  U <- mclapply(features.uniq, function(feature) {

    print(feature)

    mat <- cbind(as(pl$genotypes[, snp.eval.split[[feature]]], 'numeric'),
                 covar)

    fit <- glmnet(mat,
                  response,
                  alpha=1,
                  lambda = cv.fits[[feature]]$lambda.1se,
                  family='binomial')

    devi <- lapply(seq_len(nperm), function(ix){

      # new design matrix with shuffled genotypes
      mat <- cbind(as(pl$genotypes[perm.index[,ix], snp.eval.split[[feature]]], 'numeric'),
                 covar)

      # Use shuffled response here
      f <- cv.glmnet(mat,
                     response,
                     alpha=1,
                     #nfolds = 100, # too slow
                     family='binomial')
      glmnet(mat,
             response,
             alpha=1,
             lambda = f$lambda.1se,
             family='binomial')
    })

    list(fit=fit, devi=devi)
  }, mc.cores = 12)

  names(U) <- features.uniq
  main.model.dev.ratio <- sapply(U, function(feature)feature$fit$dev.ratio)
  D <- sapply(U, function(feature)sapply(feature$devi, function(permfit)permfit$dev.ratio))
  D <- t(D)

  saveRDS(U, 'data/permutation_models.Rds')
  saveRDS(D, 'data/permutation_models_devratios.Rds')
  saveRDS(main.model.dev.ratio, 'data/models_devratios.Rds')
}
```

For all 1000 permutations and 919 models, `U` matrix ($1000\times919$) stores the deviance values for each fitted random model. Because this also takes significant amount of time, we use a pre-computed matrix.

#### Sort models and calculate empirical p-values

```{r calculateempiricalpvals}
#perform model selection only on models with non-zero coef.
nonzero.models <- names(fits.coeff.len)[fits.coeff.len > 0]
main.model.dev.ratio <- sort(main.model.dev.ratio[nonzero.models], decreasing = T)
D <- D[names(main.model.dev.ratio),]

qplot(main.model.dev.ratio, bins=60, color=I('white')) + theme_minimal()
#pheatmap(D)

p.values <- sapply(rownames(D), function(m)mean(D[m,] >= main.model.dev.ratio[m]))
names(p.values) <- names(main.model.dev.ratio)
p.values.adj <- p.adjust(p.values, 'fdr')
head(p.values.adj)
```


#### Summarize everything as a tsv file

```{r generatesummaryfile}
# create a dataframe with all models, pvalues and deviance values
failed.lasso.names <- setdiff(features.uniq, names(p.values.adj))
failed.lasso.pvals <- rep(NA_real_, length(failed.lasso.names))
names(failed.lasso.pvals) <- failed.lasso.names

#combine failed lasso models and observed pvalues
sigmodel.pvalues <- c(p.values.adj, failed.lasso.pvals)
sigmodel.pvalues.orig <- c(p.values, failed.lasso.pvals)
sigmodel.names <- features[match(names(sigmodel.pvalues), features.uniq)]
sigmodel.names.uniq <- features.uniq[match(names(sigmodel.pvalues), features.uniq)]
sigmodel.df <- as.data.frame(do.call(rbind, strsplit(sigmodel.names, '|', fixed = T)), stringsAsFactors = F)
colnames(sigmodel.df) <- c('Cell.line', 'Chromatin.feature', 'Treatment')
sigmodel.df$pval.adj <- sigmodel.pvalues
sigmodel.df$pval <- sigmodel.pvalues.orig
sigmodel.df$codename <- sigmodel.names.uniq
sigmodel.df$deepsea.name <- sigmodel.names
```


```{r}
sigmodel.df$Treatment <- gsub('None', '', gsub('\\.[1-9]$', '', sigmodel.df$Treatment))
sigmodel.df$devratio <- main.model.dev.ratio[match(sigmodel.df$codename, names(main.model.dev.ratio))]
sigmodel.df <- sigmodel.df[order(sigmodel.df$pval.adj),]
rownames(sigmodel.df) <- NULL
head(select(sigmodel.df, -codename, -deepsea.name), 12)
write.table(sigmodel.df, 'data/permutation_pvalues.tsv', sep='\t', quote = F, row.names = F)
```

#### Visualize variable and model selection

```{r visualizevariableandmodelselection, fig.height=12, fig.width=15}
# now, plot the pvalue heatmap --------------------------------------------
pvalue.heatmap <- sigmodel.df
pvalue.heatmap$Treatment[pvalue.heatmap$Treatment !=''] <- paste0('(', pvalue.heatmap$Treatment[pvalue.heatmap$Treatment!=''], ')')
pvalue.heatmap <- dplyr::select(pvalue.heatmap, Cell.line, Chromatin.feature, Treatment, pval.adj)

#pick smallest p-value if multiple models with the same name exist
pvalue.heatmap <- dplyr::group_by(pvalue.heatmap, Cell.line, Chromatin.feature, Treatment) %>%
  dplyr::mutate(pval.adj=min(pval.adj, na.rm=T)) %>%
  dplyr::distinct(.keep_all = T) %>%
  as.data.frame(., stringsAsFactors=F)

pvalue.heatmap$pval.adj[is.na(pvalue.heatmap$pval.adj)] <- -2

#fill `not measured` with NA so that we can color it differently
pvalue.heatmap <- spread(pvalue.heatmap, "Chromatin.feature", pval.adj)
rownames(pvalue.heatmap) <- paste0(pvalue.heatmap$Cell.line, pvalue.heatmap$Treatment)
pvalue.heatmap$Cell.line <- NULL
pvalue.heatmap$Treatment <- NULL

f <- function(x){(!is.na(x)) & (x != -2) & (x < 0.1)}
pvalue.heatmap <- pvalue.heatmap[rowSums((!f(pvalue.heatmap)), na.rm = T) != ncol(pvalue.heatmap),]
pvalue.heatmap <- pvalue.heatmap[,colSums((!f(pvalue.heatmap)), na.rm = T) != nrow(pvalue.heatmap)]

#sort the rows based on the total number of significant models
ord <- order(rowSums(f(pvalue.heatmap), na.rm = T), decreasing = T)

#the models that have no non-zero lasso coef. and the ones that failed the significance test are all
#represented by the same color
#set the pvalue of the lasso-dropped models (previously set to 1) somewhere barely above 0.1 so that it'll be painted in gray
pvalue.heatmap[pvalue.heatmap == -2] <- 0.1001

heatmap.2(as.matrix(pvalue.heatmap[ord,]),
          Rowv = F, Colv = F, trace = 'none', dendrogram = 'none', na.color = 'white',
          col=c(colorRampPalette(rev(brewer.pal(9, 'YlOrRd')[-1]),
                                 interpolate='spline')(20), 'gray'),
          breaks=c(seq(0, 0.1, length.out = 21), max(pvalue.heatmap, na.rm = T)),
          cexRow = 1,
          cexCol = 1,
          keysize = 1,
          key.title = 'FDR-adjusted p-value',
          key.xlab = 'FDR-adjusted p-value',
          density.info='none',
          sepwidth=c(0.005,0.005),
          sepcolor="black",
          colsep=c(seq(0,ncol(pvalue.heatmap), 5), ncol(pvalue.heatmap)),
          rowsep=c(seq(0, nrow(pvalue.heatmap), 5), nrow(pvalue.heatmap)),
          margins = c(7,17)
)
```
